{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the user configs\n",
    "with open('conf/conf.json') as f:\n",
    "  config = json.load(f)\n",
    "\n",
    "# config variables\n",
    "test_size     = config[\"test_size\"]\n",
    "seed      = config[\"seed\"]\n",
    "features_path   = config[\"features_path\"]\n",
    "labels_path   = config[\"labels_path\"]\n",
    "results     = config[\"results\"]\n",
    "classifier_path = config[\"classifier_path\"]\n",
    "train_path    = config[\"train_path\"]\n",
    "num_classes   = config[\"num_classes\"]\n",
    "classifier_path = config[\"classifier_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import features and labels\n",
    "h5f_data  = h5py.File(features_path, 'r')\n",
    "h5f_label = h5py.File(labels_path, 'r')\n",
    "\n",
    "features_string = h5f_data['dataset_1']\n",
    "labels_string   = h5f_label['dataset_1']\n",
    "\n",
    "features = np.array(features_string)\n",
    "labels   = np.array(labels_string)\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the shape of features and labels\n",
    "print(\"[INFO] features shape: {}\".format(features.shape))\n",
    "print(\"[INFO] labels shape: {}\".format(labels.shape))\n",
    "\n",
    "print(\"[INFO] training started...\")\n",
    "# split the training and testing data\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(np.array(features),\n",
    "                                                                  np.array(labels),\n",
    "                                                                  test_size=test_size,\n",
    "                                                                  random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] splitted train and test data...\")\n",
    "print(\"[INFO] train data  : {}\".format(trainData.shape))\n",
    "print(\"[INFO] test data   : {}\".format(testData.shape))\n",
    "print(\"[INFO] train labels: {}\".format(trainLabels.shape))\n",
    "print(\"[INFO] test labels : {}\".format(testLabels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use logistic regression as the model\n",
    "print(\"[INFO] creating model...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression(random_state=seed)\n",
    "model = LogisticRegression(random_state=seed, solver='lbfgs',dual=False, max_iter=4000)\n",
    "model.fit(trainData, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use rank-1 and rank-5 predictions\n",
    "print(\"[INFO] evaluating model...\")\n",
    "f = open(results, \"w\")\n",
    "rank_1 = 0\n",
    "rank_5 = 0\n",
    "\n",
    "# loop over test data\n",
    "for (label, features) in zip(testLabels, testData):\n",
    "  # predict the probability of each class label and\n",
    "  # take the top-5 class labels\n",
    "  predictions = model.predict_proba(np.atleast_2d(features))[0]\n",
    "  predictions = np.argsort(predictions)[::-1][:5]\n",
    "\n",
    "  # rank-1 prediction increment\n",
    "  if label == predictions[0]:\n",
    "    rank_1 += 1\n",
    "\n",
    "  # rank-5 prediction increment\n",
    "  if label in predictions:\n",
    "    rank_5 += 1\n",
    "\n",
    "# convert accuracies to percentages\n",
    "rank_1 = (rank_1 / float(len(testLabels))) * 100\n",
    "rank_5 = (rank_5 / float(len(testLabels))) * 100\n",
    "\n",
    "# write the accuracies to file\n",
    "f.write(\"Rank-1: {:.2f}%\\n\".format(rank_1))\n",
    "f.write(\"Rank-5: {:.2f}%\\n\\n\".format(rank_5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model of test data\n",
    "preds = model.predict(testData)\n",
    "\n",
    "# write the classification report to file\n",
    "f.write(\"{}\\n\".format(classification_report(testLabels, preds)))\n",
    "f.close()\n",
    "\n",
    "# dump classifier to file\n",
    "print(\"[INFO] saving model...\")\n",
    "pickle.dump(model, open(classifier_path, 'wb'))\n",
    "\n",
    "# display the confusion matrix\n",
    "print(\"[INFO] confusion matrix\")\n",
    "\n",
    "# get the list of training lables\n",
    "labels = sorted(list(os.listdir(train_path)))\n",
    "\n",
    "# plot the confusion matrix\n",
    "cm = confusion_matrix(testLabels, preds)\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            cmap=\"Set2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
